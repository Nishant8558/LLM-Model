{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "6d607e8ecb0d4ee7b04941b214b2036c",
            "4e8141a0a103433bad087cf928f0719e",
            "67817e888d0d4772a4c2d089c60bf796",
            "beb06d7df3564b9ebb75b2869ef6e3c7",
            "a99804faa11c419b8bfdcd782513c1c5",
            "2c445175ed02475d8d2bfc29d23cb0a8",
            "8453e96c93d5435ba155575e1649fa4e"
          ]
        },
        "id": "KKN_pAPHcZ5s",
        "outputId": "851edd92-942f-4b63-c924-e5b541719ce3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d607e8ecb0d4ee7b04941b214b2036c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e8141a0a103433bad087cf928f0719e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67817e888d0d4772a4c2d089c60bf796",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "beb06d7df3564b9ebb75b2869ef6e3c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a99804faa11c419b8bfdcd782513c1c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c445175ed02475d8d2bfc29d23cb0a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8453e96c93d5435ba155575e1649fa4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt Engineering Monitoring\n",
            "Threshold-based latency still applies per generation.\n",
            "\n",
            "Base: Explain LangChain in one line.\n",
            "Variation: direct  |  Latency: 1.741s  (EXCEEDED)\n",
            "Generated:\n",
            " a syllable of a word\n",
            "Heuristic scores -> keyword: 0.0  length: 0.4578  combined: 0.1831\n",
            "----------------------------------------------------------------------\n",
            "Base: Explain LangChain in one line.\n",
            "Variation: contextual  |  Latency: 1.330s  (WITHIN_LIMIT)\n",
            "Generated:\n",
            " LangChain is a generative machine learning algorithm.\n",
            "Heuristic scores -> keyword: 0.0  length: 0.4868  combined: 0.1947\n",
            "----------------------------------------------------------------------\n",
            "Base: Explain LangChain in one line.\n",
            "Variation: constrained  |  Latency: 2.045s  (EXCEEDED)\n",
            "Generated:\n",
            " LangChain is a physicist who specializes in physics.\n",
            "Heuristic scores -> keyword: 0.0  length: 0.4111  combined: 0.1644\n",
            "----------------------------------------------------------------------\n",
            "Base: Explain LangGraph simply.\n",
            "Variation: direct  |  Latency: 1.682s  (EXCEEDED)\n",
            "Generated:\n",
            " LangGraph is a graph that is a graph of the graph's shape.\n",
            "Heuristic scores -> keyword: 0.2  length: 0.8007  combined: 0.4403\n",
            "----------------------------------------------------------------------\n",
            "Base: Explain LangGraph simply.\n",
            "Variation: contextual  |  Latency: 1.574s  (EXCEEDED)\n",
            "Generated:\n",
            " LangGraph is a graph that is used to visualize the graph of a graph.\n",
            "Heuristic scores -> keyword: 0.2  length: 0.7642  combined: 0.4257\n",
            "----------------------------------------------------------------------\n",
            "Base: Explain LangGraph simply.\n",
            "Variation: constrained  |  Latency: 1.483s  (WITHIN_LIMIT)\n",
            "Generated:\n",
            " LangGraph is a graph that is used to visualize the graph.\n",
            "Heuristic scores -> keyword: 0.2  length: 0.4819  combined: 0.3128\n",
            "----------------------------------------------------------------------\n",
            "Base: What is MLOps for LLMs?\n",
            "Variation: direct  |  Latency: 1.496s  (WITHIN_LIMIT)\n",
            "Generated:\n",
            " a rewrite of the original\n",
            "Heuristic scores -> keyword: 0.0  length: 0.4578  combined: 0.1831\n",
            "----------------------------------------------------------------------\n",
            "Base: What is MLOps for LLMs?\n",
            "Variation: contextual  |  Latency: 1.023s  (WITHIN_LIMIT)\n",
            "Generated:\n",
            " a machine learning algorithm\n",
            "Heuristic scores -> keyword: 0.0  length: 0.3753  combined: 0.1501\n",
            "----------------------------------------------------------------------\n",
            "Base: What is MLOps for LLMs?\n",
            "Variation: constrained  |  Latency: 1.318s  (WITHIN_LIMIT)\n",
            "Generated:\n",
            " MLOps for LLMs.\n",
            "Heuristic scores -> keyword: 0.0  length: 0.3047  combined: 0.1219\n",
            "----------------------------------------------------------------------\n",
            "Base: Translate to Hindi: Hello, how are you?\n",
            "Variation: direct  |  Latency: 0.854s  (WITHIN_LIMIT)\n",
            "Generated:\n",
            " ,  ?\n",
            "Heuristic scores -> keyword: 0.0  length: 0.3247  combined: 0.1299\n",
            "----------------------------------------------------------------------\n",
            "Base: Translate to Hindi: Hello, how are you?\n",
            "Variation: contextual  |  Latency: 0.881s  (WITHIN_LIMIT)\n",
            "Generated:\n",
            " ,  ?\n",
            "Heuristic scores -> keyword: 0.0  length: 0.3086  combined: 0.1235\n",
            "----------------------------------------------------------------------\n",
            "Base: Translate to Hindi: Hello, how are you?\n",
            "Variation: constrained  |  Latency: 6.302s  (EXCEEDED)\n",
            "Generated:\n",
            " ,  ?                                    \n",
            "Heuristic scores -> keyword: 0.0  length: 0.2855  combined: 0.1142\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Recommendation summary (best variation per prompt by combined_score):\n",
            "\n",
            "- Explain LangChain in one line.\n",
            "  Best variation: contextual  (score 0.1947)\n",
            "  Generated (first 120 chars): LangChain is a generative machine learning algorithm.\n",
            "  Why: fast enough\n",
            "\n",
            "- Explain LangGraph simply.\n",
            "  Best variation: direct  (score 0.4403)\n",
            "  Generated (first 120 chars): LangGraph is a graph that is a graph of the graph's shape.\n",
            "  Why: matched important keywords, sensible length\n",
            "\n",
            "- What is MLOps for LLMs?\n",
            "  Best variation: direct  (score 0.1831)\n",
            "  Generated (first 120 chars): a rewrite of the original\n",
            "  Why: fast enough\n",
            "\n",
            "- Translate to Hindi: Hello, how are you?\n",
            "  Best variation: direct  (score 0.1299)\n",
            "  Generated (first 120 chars): ,  ?\n",
            "  Why: fast enough\n",
            "\n",
            "Full results saved to: prompt_variation_results.csv\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import csv\n",
        "import math\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "device = \"cuda\" if __import__(\"torch\").cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "def scoring_function(prompt, max_new_tokens=80):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def make_variations(base):\n",
        "    variations = []\n",
        "    variations.append((\"direct\", base, 20, []))\n",
        "    variations.append((\"contextual\", \"You are an expert AI instructor. Answer concisely and clearly:\\n\\n\" + base, 25, []))\n",
        "    variations.append((\"constrained\", base + \"\\n\\nGive 3 short numbered points (1-2 sentences each) and a one-line summary at the end.\", 40, []))\n",
        "    return variations\n",
        "\n",
        "def keyword_score(text, keywords):\n",
        "    if not keywords:\n",
        "        return 0.0\n",
        "    text_low = text.lower()\n",
        "    hits = sum(1 for k in keywords if k.lower() in text_low)\n",
        "    return hits / len(keywords)\n",
        "\n",
        "def length_score(text, target_tokens):\n",
        "    tokens = len(text.split())\n",
        "    sigma = max(3, target_tokens * 0.6)\n",
        "    return math.exp(-0.5 * ((tokens - target_tokens) / sigma) ** 2)\n",
        "\n",
        "def combined_score(text, keywords, target_tokens):\n",
        "    ks = keyword_score(text, keywords)\n",
        "    ls = length_score(text, target_tokens)\n",
        "    if not keywords:\n",
        "        return 0.35 * ls + 0.65 * min(1.0, len(text) / max(1, target_tokens * 4))\n",
        "    return 0.6 * ks + 0.4 * ls\n",
        "\n",
        "keyword_guidance = {\n",
        "    \"Explain LangChain in one line.\": [\"chains\", \"agent\", \"component\", \"LLM\", \"prompts\"],\n",
        "    \"Explain LangGraph simply.\": [\"graph\", \"nodes\", \"edges\", \"flow\", \"pipeline\"],\n",
        "    \"What is MLOps for LLMs?\": [\"deployment\", \"monitoring\", \"model\", \"data\", \"scaling\"],\n",
        "    \"Translate to Hindi: Hello, how are you?\": [\"नमस्ते\", \"कैसे\", \"हो\"]\n",
        "}\n",
        "\n",
        "base_prompts = [\n",
        "    \"Explain LangChain in one line.\",\n",
        "    \"Explain LangGraph simply.\",\n",
        "    \"What is MLOps for LLMs?\",\n",
        "    \"Translate to Hindi: Hello, how are you?\"\n",
        "]\n",
        "\n",
        "output_csv = \"prompt_variation_results.csv\"\n",
        "\n",
        "all_results = []\n",
        "print(\"\\nPrompt Engineering Monitoring\\n\")\n",
        "\n",
        "THRESHOLD = 1.5\n",
        "\n",
        "for base in base_prompts:\n",
        "    variations = make_variations(base)\n",
        "    keywords = keyword_guidance.get(base, [])\n",
        "    for i, (vname, vprompt, target_len, _) in enumerate(variations):\n",
        "        v_keywords = keywords\n",
        "        start = time.perf_counter()\n",
        "        try:\n",
        "            gen = scoring_function(vprompt, max_new_tokens=80)\n",
        "        except Exception as e:\n",
        "            gen = f\"[ERROR: {e}]\"\n",
        "        latency = time.perf_counter() - start\n",
        "        status = \"WITHIN_LIMIT\" if latency <= THRESHOLD else \"EXCEEDED\"\n",
        "        score = combined_score(gen, v_keywords, target_len)\n",
        "\n",
        "        row = {\n",
        "            \"base_prompt\": base,\n",
        "            \"variation\": vname,\n",
        "            \"prompt_text\": vprompt,\n",
        "            \"generated\": gen,\n",
        "            \"latency_sec\": round(latency, 3),\n",
        "            \"latency_status\": status,\n",
        "            \"keyword_hits\": keyword_score(gen, v_keywords),\n",
        "            \"length_score\": round(length_score(gen, target_len), 4),\n",
        "            \"combined_score\": round(score, 4)\n",
        "        }\n",
        "        all_results.append(row)\n",
        "\n",
        "        print(f\"Base: {base}\")\n",
        "        print(f\"Variation: {vname}  |  Latency: {latency:.3f}s  ({status})\")\n",
        "        print(\"Generated:\\n\", gen)\n",
        "        print(\"Heuristic scores -> keyword:\", row[\"keyword_hits\"], \" length:\", row[\"length_score\"], \" combined:\", row[\"combined_score\"])\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "fieldnames = [\"base_prompt\",\"variation\",\"prompt_text\",\"generated\",\"latency_sec\",\"latency_status\",\"keyword_hits\",\"length_score\",\"combined_score\"]\n",
        "\n",
        "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for r in all_results:\n",
        "        writer.writerow({k: r.get(k, \"\") for k in fieldnames})\n",
        "\n",
        "print(\"\\nRecommendation summary:\\n\")\n",
        "\n",
        "for base in base_prompts:\n",
        "    rows = [r for r in all_results if r[\"base_prompt\"] == base]\n",
        "    best = max(rows, key=lambda r: r[\"combined_score\"])\n",
        "    print(f\"- {base}\")\n",
        "    print(f\"  Best variation: {best['variation']}  (score {best['combined_score']})\")\n",
        "    print(f\"  Generated (first 120 chars): {best['generated'][:120].replace('\\\\n',' ')}\")\n",
        "    reasons = []\n",
        "    if best[\"keyword_hits\"] > 0:\n",
        "        reasons.append(\"matched important keywords\")\n",
        "    if best[\"length_score\"] > 0.6:\n",
        "        reasons.append(\"sensible length\")\n",
        "    if best[\"latency_status\"] == \"WITHIN_LIMIT\":\n",
        "        reasons.append(\"fast enough\")\n",
        "    print(\"  Why:\", \", \".join(reasons) if reasons else \"Preferable structure/clarity.\")\n",
        "    print()\n",
        "\n",
        "print(f\"Full results saved to: {output_csv}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}