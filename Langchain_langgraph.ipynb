{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrT0B20dBE1C",
        "outputId": "51563b11-a91f-417d-8b05-17dcb880b92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AI is a graphical representation of a computer.\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain langchain-core langchain-community langgraph transformers accelerate sentencepiece torch --quiet\n",
        "!pip install huggingface-hub --quiet\n",
        "\n",
        "import importlib\n",
        "importlib.invalidate_caches()\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict\n",
        "\n",
        "MODEL = \"google/flan-t5-small\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL, device_map=\"auto\")\n",
        "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=256, do_sample=False)\n",
        "else:\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
        "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=256, do_sample=False, device=-1)\n",
        "\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"Conversation:\\n{history}\\nUser: {input}\\nAssistant:\")\n",
        "\n",
        "chain = (\n",
        "    {\"history\": RunnablePassthrough(), \"input\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke({\"history\": \"\", \"input\": \"Explain overfitting in one sentence.\"}))\n",
        "\n",
        "class State(TypedDict):\n",
        "    input: str\n",
        "    response: str\n",
        "\n",
        "def llm_node(state: State):\n",
        "    out = chain.invoke({\"history\": \"\", \"input\": state[\"input\"]})\n",
        "    return {\"response\": out}\n",
        "\n",
        "def output_node(state: State):\n",
        "    return {\"response\": state[\"response\"]}\n",
        "\n",
        "graph = StateGraph(State)\n",
        "graph.add_node(\"llm\", llm_node)\n",
        "graph.add_node(\"out\", output_node)\n",
        "graph.add_edge(START, \"llm\")\n",
        "graph.add_edge(\"llm\", \"out\")\n",
        "graph.add_edge(\"out\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "result = app.invoke({\"input\": \"What is AI?\"})\n",
        "print(result[\"response\"])\n"
      ]
    }
  ]
}