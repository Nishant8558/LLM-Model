{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Ol9IgFYesH",
        "outputId": "23862bb5-e27a-43d2-b929-9c3d8b6f8a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT: 12 + 8\n",
            "OUTPUT: Result = 20\n",
            "--------------------------------------------------\n",
            "INPUT: calculate 15 * (2 + 3)\n",
            "OUTPUT: Result = 75\n",
            "--------------------------------------------------\n",
            "INPUT: What is 2^10?\n",
            "OUTPUT: Result = 1024\n",
            "--------------------------------------------------\n",
            "INPUT: What is 7+10-19\n",
            "OUTPUT: Result = -2\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langgraph langchain huggingface_hub\n",
        "\n",
        "import os, re, ast\n",
        "from typing import TypedDict\n",
        "from huggingface_hub import InferenceClient\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\") or \"YOUR_HF_TOKEN_HERE\"\n",
        "MODEL_ID = \"TeichAI/Qwen3-1.7B-Gemini-2.5-Flash-Lite-Preview-Distill\"\n",
        "client = InferenceClient(MODEL_ID, token=HF_TOKEN)\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    input: str\n",
        "    output: str\n",
        "\n",
        "ALLOWED_NODES = (\n",
        "    ast.Expression, ast.BinOp, ast.UnaryOp, ast.Constant,\n",
        "    ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow, ast.Mod,\n",
        "    ast.FloorDiv, ast.UAdd, ast.USub, ast.Load, ast.Tuple,\n",
        "    ast.List, ast.BinOp, ast.UnaryOp, ast.Expr\n",
        ")\n",
        "\n",
        "def safe_eval(expr: str):\n",
        "    node = ast.parse(expr, mode=\"eval\")\n",
        "    for nd in ast.walk(node):\n",
        "        if isinstance(nd, ast.Call):\n",
        "            raise ValueError(\"Function calls not allowed\")\n",
        "        if not isinstance(nd, ALLOWED_NODES):\n",
        "            raise ValueError(f\"Unsupported element: {type(nd).__name__}\")\n",
        "    compiled = compile(node, \"<string>\", \"eval\")\n",
        "    return eval(compiled, {\"__builtins__\": {}}, {})\n",
        "\n",
        "def router_node(state: GraphState):\n",
        "    text = state[\"input\"].strip()\n",
        "    math_pattern = re.search(r\"(?:^|\\b)(calculate|what is|evaluate)\\b\", text, flags=re.I) \\\n",
        "                   or re.search(r\"\\d+\\s*[\\+\\-\\*\\/\\^%]\\s*\\d+\", text)\n",
        "    if math_pattern:\n",
        "        return {\"route\": \"calculator\"}\n",
        "    return {\"route\": \"llm\"}\n",
        "\n",
        "def calculator_node(state: GraphState):\n",
        "    text = state[\"input\"].strip()\n",
        "    expr = re.sub(r\"(calculate|what is|evaluate|please|=|\\?)\", \"\", text, flags=re.I).strip()\n",
        "    expr = expr.replace(\"^\", \"**\")\n",
        "    try:\n",
        "        result = safe_eval(expr)\n",
        "        return {\"output\": f\"Result = {result}\"}\n",
        "    except Exception:\n",
        "        m = re.search(r\"[\\d\\.\\s\\(\\)\\+\\-\\*\\/\\*\\*%]+\", text.replace(\"^\", \"**\"))\n",
        "        if m:\n",
        "            candidate = m.group(0).replace(\"^\", \"**\")\n",
        "            try:\n",
        "                result = safe_eval(candidate)\n",
        "                return {\"output\": f\"Result = {result}\"}\n",
        "            except Exception as e:\n",
        "                return {\"output\": f\"Invalid or unsupported math expression. ({e})\"}\n",
        "        return {\"output\": \"No valid math expression found.\"}\n",
        "\n",
        "def llm_node(state: GraphState):\n",
        "    prompt = state[\"input\"].strip()\n",
        "    try:\n",
        "        resp = client.text_generation({\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 150}})\n",
        "    except Exception as e:\n",
        "        return {\"output\": f\"LLM error: {repr(e)}\"}\n",
        "    generated = None\n",
        "    try:\n",
        "        if isinstance(resp, dict):\n",
        "            if \"generated_text\" in resp:\n",
        "                generated = resp[\"generated_text\"]\n",
        "            elif \"outputs\" in resp and isinstance(resp[\"outputs\"], list) and resp[\"outputs\"]:\n",
        "                first = resp[\"outputs\"][0]\n",
        "                generated = first.get(\"generated_text\") or first.get(\"text\") or str(first)\n",
        "            elif \"data\" in resp and isinstance(resp[\"data\"], list) and resp[\"data\"]:\n",
        "                first = resp[\"data\"][0]\n",
        "                generated = first.get(\"generated_text\") or first.get(\"text\") or str(first)\n",
        "            else:\n",
        "                generated = str(resp)\n",
        "        elif isinstance(resp, list) and resp:\n",
        "            first = resp[0]\n",
        "            if isinstance(first, dict):\n",
        "                generated = first.get(\"generated_text\") or first.get(\"text\") or str(first)\n",
        "            else:\n",
        "                generated = str(first)\n",
        "        else:\n",
        "            generated = str(resp)\n",
        "    except Exception as e:\n",
        "        generated = f\"Failed to parse LLM response: {repr(e)}\"\n",
        "    if not generated:\n",
        "        generated = str(resp)\n",
        "    return {\"output\": generated}\n",
        "\n",
        "graph = StateGraph(GraphState)\n",
        "graph.add_node(\"router\", router_node)\n",
        "graph.add_node(\"calculator\", calculator_node)\n",
        "graph.add_node(\"llm\", llm_node)\n",
        "graph.set_entry_point(\"router\")\n",
        "graph.add_conditional_edges(\"router\", lambda state: state[\"route\"], {\"calculator\": \"calculator\", \"llm\": \"llm\"})\n",
        "graph.add_edge(\"calculator\", END)\n",
        "graph.add_edge(\"llm\", END)\n",
        "app = graph.compile()\n",
        "\n",
        "tests = [\n",
        "    \"12 + 8\",\n",
        "    \"calculate 15 * (2 + 3)\",\n",
        "    \"What is 2^10?\",\n",
        "    \"What is 7+10-19\"\n",
        "]\n",
        "\n",
        "for t in tests:\n",
        "    out = app.invoke({\"input\": t})\n",
        "    print(\"INPUT:\", t)\n",
        "    print(\"OUTPUT:\", out.get(\"output\"))\n",
        "    print(\"-\" * 50)\n"
      ]
    }
  ]
}